{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Models\n",
    "\n",
    "You can use Azure Machine Learning to interpret a model by using an *explainer* that quantifies the amount of influence each feature contribues to the predicted label. There are many common explainers, each suitable for different kinds of modeling algorithm; but the basic approach to using them is the same.\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "Before you start this lab, ensure that you have completed the *Create an Azure Machine Learning Workspace* and *Create a Compute Instance* tasks in [Lab 1: Getting Started with Azure Machine Learning](./labdocs/Lab01.md). Then open this notebook in Jupyter on your Compute Instance.\n",
    "\n",
    "Then run the following cell to ensure that you have the latest version of the Azure ML SDK installed, and in addition install the Azure ML Interpretability library. You can use this to interpret many typical kinds of model, even if they haven't been trained in an Azure ML experiment or registered in an Azure ML workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade azureml-sdk[notebooks,automl,explain]\n",
    "!pip install --upgrade azureml-interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain a Model\n",
    "\n",
    "Let's start with a model that is trained outside of Azure Machine Learning - Run the cell below to train a decision tree classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# load the diabetes dataset\n",
    "print(\"Loading Data...\")\n",
    "data = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "features = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']\n",
    "labels = ['not-diabetic', 'diabetic']\n",
    "X, y = data[features].values, data['Diabetic'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a decision tree model\n",
    "print('Training a decision tree model')\n",
    "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "\n",
    "print('Model trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process generated some model evaluation metrics based on a hold-back validation dataset, so you have an idea of how accurately it predicts; but how do the features in the data influence the prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an Explainer for our Model\n",
    "\n",
    "Let's get a suitable explainer for the model from the Azure ML interpretability library you installed earlier. There are many kinds of explainer. In this example you'll use a *Tabular Explainer*, which is a \"black box\" explainer that can be used to explain many kinds of model by invoking an appropriate [SHAP](https://github.com/slundberg/shap) model explainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.ext.blackbox import TabularExplainer\n",
    "\n",
    "# \"features\" and \"classes\" fields are optional\n",
    "tab_explainer = TabularExplainer(model, \n",
    "                             X_train, \n",
    "                             features=features, \n",
    "                             classes=labels)\n",
    "print(tab_explainer, \"ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Global Feature Importance\n",
    "\n",
    "The first thing to do is try to explain the model by evaluating the overall *feature importance* - in other words, quantifying the extent to which each feature influences the prediction based on the whole training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use the training data or the test data here\n",
    "global_tab_explanation = tab_explainer.explain_global(X_train)\n",
    "\n",
    "# Get the top features by importance\n",
    "global_tab_feature_importance = global_tab_explanation.get_feature_importance_dict()\n",
    "for feature, importance in global_tab_feature_importance.items():\n",
    "    print(feature,\":\", importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance is ranked, with the most important feature listed first.\n",
    "\n",
    "### Get Local Feature Importance\n",
    "\n",
    "So you have an overall view, but what about explaining individual observations? Let's generate *local* explanations for individual predictions, quantifying the extent to which each feature influenced the decision to predict each of the possible label values. In this case, it's a binary model, so there are two possible labels (non-diabetic and diabetic); and you can quantify the influence of each feature for each of these label values for individual observations in a dataset. You'll just evaluate the first two cases in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the observations we want to explain (the first two)\n",
    "X_explain = X_test[0:2]\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(X_explain)\n",
    "\n",
    "# Get local explanations\n",
    "local_tab_explanation = tab_explainer.explain_local(X_explain)\n",
    "\n",
    "# Get feature names and importance for each possible label\n",
    "local_tab_features = local_tab_explanation.get_ranked_local_names()\n",
    "local_tab_importance = local_tab_explanation.get_ranked_local_values()\n",
    "\n",
    "for l in range(len(local_tab_features)):\n",
    "    print('Support for', labels[l])\n",
    "    label = local_tab_features[l]\n",
    "    for o in range(len(label)):\n",
    "        print(\"\\tObservation\", o + 1)\n",
    "        feature_list = label[o]\n",
    "        total_support = 0\n",
    "        for f in range(len(feature_list)):\n",
    "            print(\"\\t\\t\", feature_list[f], ':', local_tab_importance[l][o][f])\n",
    "            total_support += local_tab_importance[l][o][f]\n",
    "        print(\"\\t\\t ----------\\n\\t\\t Total:\", total_support, \"Prediction:\", labels[predictions[o]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Explainability to Azure ML Models Training Experiments\n",
    "\n",
    "As you've seen, you can generate explanations for models trained outside of Azure ML; but when you use experiments to train models in your Azure ML workspace, you can generate model explanations and log them.\n",
    "\n",
    "### Connect to Your Workspace\n",
    "\n",
    "To run an experiment, you need to connect to your workspace using the Azure ML SDK.\n",
    "\n",
    "> **Note**: You may be prompted to authenticate. Just copy the code and click the link provided to sign into your Azure subscription, and then return to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Explain a Model using an Experiment\n",
    "\n",
    "OK, let's create an experiment and put the files it needs in a local folder - in this case we'll just use the same CSV file of diabetes data to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "experiment_folder = 'diabetes_train_and_explain'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "# Copy the data file into the experiment folder\n",
    "shutil.copy('data/diabetes.csv', os.path.join(experiment_folder, \"diabetes.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a training script that looks similar to any other Azure ML training script except that is includes the following features:\n",
    "\n",
    "- The same libraries to generate model explanations we used before are imported and used to generate a global explanation\n",
    "- The **ExplanationClient** library is used to upload the explanation to the experiment output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/diabetes_training.py\n",
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Import Azure ML run library\n",
    "from azureml.core.run import Run\n",
    "\n",
    "# Import libraries for model explanation\n",
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the diabetes dataset\n",
    "print(\"Loading Data...\")\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "features = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']\n",
    "labels = ['not-diabetic', 'diabetic']\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = data[features].values, data['Diabetic'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a decision tree model\n",
    "print('Training a decision tree model')\n",
    "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=model, filename='outputs/diabetes.pkl')\n",
    "\n",
    "# Get explanation\n",
    "explainer = TabularExplainer(model, X_train, features=features, classes=labels)\n",
    "explanation = explainer.explain_global(X_test)\n",
    "\n",
    "# Get an Explanation Client and upload the explanation\n",
    "explain_client = ExplanationClient.from_run(run)\n",
    "explain_client.upload_model_explanation(explanation, comment='Tabular Explanation')\n",
    "\n",
    "# Complete the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can run the experiment, using an estimator to run the training script. Note that the **azureml-interpret** library is included in the training environment so the script can create a **TabularExplainer**, and the **azureml-contrib-interpret** package is included so the script can use the **ExplainerClient** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "env = Environment('diabetes-interpret-env')\n",
    "env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
    "env.docker.enabled = True # Use a docker container\n",
    "\n",
    "# Create a set of package dependencies (including the azureml-contrib-interpret package)\n",
    "packages = CondaDependencies.create(conda_packages=['scikit-learn','pandas'],\n",
    "                                    pip_packages=['azureml-defaults','azureml-interpret','azureml-contrib-interpret'])\n",
    "\n",
    "# Add the dependencies to the environment\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "# Create an estimator\n",
    "estimator = Estimator(source_directory=experiment_folder,\n",
    "              compute_target = 'local', # Use local compute\n",
    "              environment_definition = env,\n",
    "              entry_script='diabetes_training.py')\n",
    "\n",
    "# Run the experiment\n",
    "experiment = Experiment(workspace = ws, name = 'diabetes_train_and_explain')\n",
    "run = experiment.submit(config=estimator)\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the Feature Importance Values\n",
    "\n",
    "With the experiment run completed, you can use the **ExplanationClient** class to retrieve the feature importance from the explanation registered for the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.interpret.explanation.explanation_client import ExplanationClient\n",
    "\n",
    "# Get the feature explanations\n",
    "client = ExplanationClient.from_run(run)\n",
    "engineered_explanations = client.download_model_explanation()\n",
    "feature_importances = engineered_explanations.get_feature_importance_dict()\n",
    "\n",
    "# Overall feature importance\n",
    "print('Feature\\tImportance')\n",
    "for key, value in feature_importances.items():\n",
    "    print(key, '\\t', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Model Explanation in Azure Machine Learning studio\n",
    "\n",
    "You can also click the link in the Run Details widget to see the run in Azure Machine Learning studio, and view the **Explanations** tab. Then:\n",
    "\n",
    "1. Select the **Tabular Explanation** explainer.\n",
    "2. View the **Global Importance** chart, which shows the overall global feature importance.\n",
    "3. View the **Summary Importance** chart, which shows each data point from the test data in a *swarm*, *violin*, or *box* plot.\n",
    "4. Select an individual point to see the **Local Feature Importance** for the individual prediction for the selected data point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Information**: For more information about using explainers in Azure ML, see [the documentation](https://docs.microsoft.com/azure/machine-learning/how-to-machine-learning-interpretability). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}